"use strict";(self.webpackChunktutorial_of_ai_kit=self.webpackChunktutorial_of_ai_kit||[]).push([[115],{3449:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>d,toc:()=>a});var t=r(5893),i=r(1151);const o={sidebar_position:8},l="Distributed Inference of DeepSeek model on Raspberry Pi",d={id:"Chapter_4-Large_Language_Model/Distributed_Inference_of_DeepSeek_model_on_Raspberry_Pi",title:"Distributed Inference of DeepSeek model on Raspberry Pi",description:"Introduction",source:"@site/../articles/Chapter_4-Large_Language_Model/Distributed_Inference_of_DeepSeek_model_on_Raspberry_Pi.md",sourceDirName:"Chapter_4-Large_Language_Model",slug:"/Chapter_4-Large_Language_Model/Distributed_Inference_of_DeepSeek_model_on_Raspberry_Pi",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_4-Large_Language_Model/Distributed_Inference_of_DeepSeek_model_on_Raspberry_Pi",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:8,frontMatter:{sidebar_position:8},sidebar:"tutorialSidebar",previous:{title:"Deploy DeepSeek on Raspberry Pi AI Box",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_4-Large_Language_Model/Run_DeepSeek_on_Raspberry_Pi_AI_Box"},next:{title:"Chapter 5 - Custom Model Development and Deployment",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_5-Custom_Model_Development_and_Deployment/"}},s={},a=[{value:"Introduction",id:"introduction",level:2},{value:"Prepare Hardware",id:"prepare-hardware",level:2},{value:"Prepare software",id:"prepare-software",level:2},{value:"update the system:",id:"update-the-system",level:3},{value:"Install ditributed llama to your root and worker node",id:"install-ditributed-llama-to-your-root-and-worker-node",level:3},{value:"Run on your woker node",id:"run-on-your-woker-node",level:3},{value:"Run on your root node",id:"run-on-your-root-node",level:3},{value:"Creat and activate python vitural environment",id:"creat-and-activate-python-vitural-environment",level:4},{value:"Install necessary lib",id:"install-necessary-lib",level:4},{value:"Install deepseek 8b q40 model",id:"install-deepseek-8b-q40-model",level:4},{value:"Run distributed inference on root node",id:"run-distributed-inference-on-root-node",level:4},{value:"Result",id:"result",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",strong:"strong",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"distributed-inference-of-deepseek-model-on-raspberry-pi",children:"Distributed Inference of DeepSeek model on Raspberry Pi"}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(n.p,{children:["This wiki explains how to deploy the ",(0,t.jsx)(n.a,{href:"https://github.com/deepseek-ai/DeepSeek-LLM",children:"DeepSeek"})," model on Multiple Raspberry Pi AI Boxs with ",(0,t.jsx)(n.a,{href:"https://github.com/b4rtaz/distributed-llama",children:"distributed-llama"}),".In this wiki, I used a ",(0,t.jsx)(n.strong,{children:"Raspberry Pi with 8GB of RAM"})," as the ",(0,t.jsx)(n.strong,{children:"root node"})," and ",(0,t.jsx)(n.strong,{children:"three Raspberry Pis with 4GB of RAM"})," as ",(0,t.jsx)(n.strong,{children:"worker nodes"})," to run the ",(0,t.jsx)(n.strong,{children:"DeepSeek 8B model"}),". The inference speed reached ",(0,t.jsx)(n.strong,{children:"6.06 tokens per second"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"prepare-hardware",children:"Prepare Hardware"}),"\n",(0,t.jsx)("div",{class:"table-center",children:(0,t.jsxs)("table",{align:"center",children:[(0,t.jsx)("tr",{children:(0,t.jsx)("th",{children:"reComputer AI R2130"})}),(0,t.jsx)("tr",{children:(0,t.jsx)("td",{children:(0,t.jsx)("div",{style:{textAlign:"center"},children:(0,t.jsx)("img",{src:"https://media-cdn.seeedstudio.com/media/catalog/product/cache/bb49d3ec4ee05b6f018e93f896b8a25d/1/_/1_24_1.jpg",style:{width:600,height:"auto"}})})})}),(0,t.jsx)("tr",{children:(0,t.jsx)("td",{children:(0,t.jsx)("div",{class:"get_one_now_container",style:{textAlign:"center"},children:(0,t.jsx)("a",{class:"get_one_now_item",href:"https://www.seeedstudio.com/reComputer-AI-R2130-12-p-6368.html",children:(0,t.jsx)("strong",{children:(0,t.jsx)("span",{children:(0,t.jsx)("font",{color:"FFFFFF",size:"4",children:" Get One Now \ud83d\uddb1\ufe0f"})})})})})})})]})}),"\n",(0,t.jsx)(n.h2,{id:"prepare-software",children:"Prepare software"}),"\n",(0,t.jsx)(n.h3,{id:"update-the-system",children:"update the system:"}),"\n",(0,t.jsxs)(n.p,{children:["Open one terminal with ",(0,t.jsx)(n.code,{children:"Ctrl+Alt+T"})," and input command like below:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"sudo date -s \"$(wget -qSO- --max-redirect=0 google.com 2>&1 | grep Date: | cut -d' ' -f5-8)Z\"\nsudo apt update\nsudo apt full-upgrade\n"})}),"\n",(0,t.jsx)(n.h3,{id:"install-ditributed-llama-to-your-root-and-worker-node",children:"Install ditributed llama to your root and worker node"}),"\n",(0,t.jsxs)(n.p,{children:["Open one terminal with ",(0,t.jsx)(n.code,{children:"Ctrl+Alt+T"})," and input command like below to install ",(0,t.jsx)(n.a,{href:"https://github.com/b4rtaz/distributed-llama.git",children:"distributed-llama"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"git clone https://github.com/b4rtaz/distributed-llama.git\ncd distributed-llama\nmake dllama\nmake dllama-api\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-on-your-woker-node",children:"Run on your woker node"}),"\n",(0,t.jsx)(n.p,{children:"Then input command like below to make worker nodes working:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd distributed-llama\nsudo nice -n -20 ./dllama worker --port 9998 --nthreads 4\n"})}),"\n",(0,t.jsx)(n.h3,{id:"run-on-your-root-node",children:"Run on your root node"}),"\n",(0,t.jsx)(n.h4,{id:"creat-and-activate-python-vitural-environment",children:"Creat and activate python vitural environment"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"cd distributed-llama\npython -m venv .env\nsource .env/bin/acitvate\n"})}),"\n",(0,t.jsx)(n.h4,{id:"install-necessary-lib",children:"Install necessary lib"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"pip install numpy==1.23.5\npip install tourch=2.0.1\npip install safetensors==0.4.2\npip install sentencepiece==0.1.99\npip install transformers\n"})}),"\n",(0,t.jsx)(n.h4,{id:"install-deepseek-8b-q40-model",children:"Install deepseek 8b q40 model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"sudo mkdir model && cd model\ngit lfs install\ngit clone https://huggingface.co/b4rtaz/Llama-3_1-8B-Q40-Instruct-Distributed-Llama\n"})}),"\n",(0,t.jsx)(n.h4,{id:"run-distributed-inference-on-root-node",children:"Run distributed inference on root node"}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," ",(0,t.jsx)(n.code,{children:"--workers 10.0.0.139:9998 10.0.0.175:9998 10.0.0.124:9998"})," is the IP of the workers."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'cd ..\n./dllama chat --model ./model/dllama_model_deepseek-r1-distill-llama-8b_q40.m --tokenizer ./model/dllama_tokenizer_deepseek-r1-distill-llama-8b.t  --buffer-float-type q80 --prompt "What is 5 plus 9 minus 3?" --nthreads 4 --max-seq-len 2048 --workers 10.0.0.139:9998 10.0.0.175:9998 10.0.0.124:9998  --steps 256\n\n'})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," If you want to test the inference speed, please use the following command."]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'cd ..\n./dllama inference --model ./model/dllama_model_deepseek-r1-distill-llama-8b_q40.m --tokenizer ./model/dllama_tokenizer_deepseek-r1-distill-llama-8b.t  --buffer-float-type q80 --prompt "What is 5 plus 9 minus 3?" --nthreads 4 --max-seq-len 2048 --workers 10.0.0.139:9998 10.0.0.175:9998 10.0.0.124:9998  --steps 256\n'})}),"\n",(0,t.jsx)(n.h2,{id:"result",children:"Result"}),"\n",(0,t.jsxs)(n.p,{children:["The following is the inference of the ",(0,t.jsx)(n.a,{href:"https://huggingface.co/b4rtaz/Llama-3_1-8B-Q40-Instruct-Distributed-Llama",children:"DeepSeek Llama 8b"})," model using 4 the Raspberry Pi."]}),"\n",(0,t.jsx)("div",{align:"center",children:(0,t.jsx)("img",{width:900,src:"https://files.seeedstudio.com/wiki/distributed-inference/distributed_llama.gif"})})]})}function h(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},1151:(e,n,r)=>{r.d(n,{Z:()=>d,a:()=>l});var t=r(7294);const i={},o=t.createContext(i);function l(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);