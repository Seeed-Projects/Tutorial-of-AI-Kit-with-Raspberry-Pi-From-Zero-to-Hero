"use strict";(self.webpackChunktutorial_of_ai_kit=self.webpackChunktutorial_of_ai_kit||[]).push([[382],{7572:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var t=n(5893),i=n(1151);const r={sidebar_position:3},s="Google Gemma 2 2B",o={id:"Chapter_4-Large_Language_Model/Run_Gemma2_on_RaspberryPi",title:"Google Gemma 2 2B",description:"Introduction",source:"@site/../articles/Chapter_4-Large_Language_Model/Run_Gemma2_on_RaspberryPi.md",sourceDirName:"Chapter_4-Large_Language_Model",slug:"/Chapter_4-Large_Language_Model/Run_Gemma2_on_RaspberryPi",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_4-Large_Language_Model/Run_Gemma2_on_RaspberryPi",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Meta Llama 3.2 1B/3B",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_4-Large_Language_Model/Run_Llama_on_RaspberryPi"},next:{title:"Microsoft Phi3.5 3.8B",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_4-Large_Language_Model/Run_Phi3.5_on_Raspberryi"}},l={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Install Ollama",id:"install-ollama",level:2},{value:"Install and run gemma2",id:"install-and-run-gemma2",level:2}];function c(e){const a={a:"a",code:"code",h1:"h1",h2:"h2",img:"img",p:"p",pre:"pre",strong:"strong",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(a.h1,{id:"google-gemma-2-2b",children:"Google Gemma 2 2B"}),"\n",(0,t.jsx)(a.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsxs)(a.p,{children:["Let's install ",(0,t.jsx)(a.a,{href:"https://ollama.com/library/gemma2:2b",children:"Gemma 2"}),", a high-performing and efficient model available in three sizes: 2B, 9B, and 27B. We will install ",(0,t.jsx)(a.a,{href:"https://huggingface.co/collections/google/gemma-2-2b-release-66a20f3796a2ff2a7c76f98f",children:(0,t.jsx)(a.strong,{children:"Gemma 2 2B"})}),", a lightweight model trained with 2 trillion tokens that produce outsized results by learning from larger models through distillation. The model has 2.6 billion parameters and a Q4_0 quantization, which ends with a size of 1.6 GB. Its context window is 8,192 tokens."]}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{src:n(2969).Z+"",width:"4230",height:"1188"})}),"\n",(0,t.jsx)(a.h2,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,t.jsxs)(a.p,{children:["Please refer to this ",(0,t.jsx)(a.a,{href:"./Setup_Ollama_on_RaspberryPi",children:"article"})]}),"\n",(0,t.jsx)(a.h2,{id:"install-and-run-gemma2",children:"Install and run gemma2"}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:"ollama run gemma2:2b --verbose\n"})}),"\n",(0,t.jsx)(a.p,{children:"Running the model with the command before, we should have the Ollama prompt available for us to input a question and start chatting with the LLM model; for example,"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:">>> What is the capital of France?"})}),"\n",(0,t.jsx)(a.p,{children:"Almost immediately, we get the correct answer:"}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.code,{children:"The capital of France is **Paris**. \ud83d\uddfc"})}),"\n",(0,t.jsx)(a.p,{children:"And it' statistics."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.img,{src:n(1392).Z+"",width:"859",height:"431"})}),"\n",(0,t.jsx)(a.p,{children:"We can see that Gemma 2:2B has around the same performance as Lama 3.2:3B, but having less parameters."}),"\n",(0,t.jsx)(a.p,{children:(0,t.jsx)(a.strong,{children:"Other examples:"})}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:">>> What is the distance between Paris and Santiago, Chile?\n\nThe distance between Paris, France and Santiago, Chile is \napproximately **7,000 miles (11,267 kilometers)**. \n\nKeep in mind that this is a straight-line distance, and actual \ntravel distance can vary depending on the chosen routes and any \nstops along the way. \u2708\ufe0f`\n"})}),"\n",(0,t.jsx)(a.p,{children:"Also, a good response but less accurate than Llama3.2:3B."}),"\n",(0,t.jsx)(a.pre,{children:(0,t.jsx)(a.code,{className:"language-bash",children:">>> what is the latitude and longitude of Paris?\n\nYou got it! Here are the latitudes and longitudes of Paris, \nFrance:\n\n* **Latitude:** 48.8566\xb0 N (north)\n* **Longitude:** 2.3522\xb0 E (east) \n\nLet me know if you'd like to explore more about Paris or its \nlocation! \ud83d\uddfc\ud83c\uddeb\ud83c\uddf7 \n"})}),"\n",(0,t.jsx)(a.p,{children:"A good and accurate answer (a little more verbose than the Llama answers)."})]})}function h(e={}){const{wrapper:a}={...(0,i.a)(),...e.components};return a?(0,t.jsx)(a,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},1392:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/gemma-5679ff6dbeeb19f606c099d37d524332.png"},2969:(e,a,n)=>{n.d(a,{Z:()=>t});const t=n.p+"assets/images/gemma_2-1595015985e561b5b128777d156fabd7.png"},1151:(e,a,n)=>{n.d(a,{Z:()=>o,a:()=>s});var t=n(7294);const i={},r=t.createContext(i);function s(e){const a=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:a},e.children)}}}]);