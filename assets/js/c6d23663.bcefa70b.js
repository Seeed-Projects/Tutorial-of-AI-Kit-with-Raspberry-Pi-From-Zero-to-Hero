"use strict";(self.webpackChunktutorial_of_ai_kit=self.webpackChunktutorial_of_ai_kit||[]).push([[571],{6257:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var a=t(5893),i=t(1151);const o={sidebar_position:6},s="Ollama Python Library",r={id:"Chapter 4 - Large Language Model/Use_Ollama_with_Python",title:"Ollama Python Library",description:"Introduction",source:"@site/../articles/Chapter 4 - Large Language Model/Use_Ollama_with_Python.md",sourceDirName:"Chapter 4 - Large Language Model",slug:"/Chapter 4 - Large Language Model/Use_Ollama_with_Python",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 4 - Large Language Model/Use_Ollama_with_Python",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{sidebar_position:6},sidebar:"tutorialSidebar",previous:{title:"Multimodal Models",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 4 - Large Language Model/Run_Multimodal_on_Raspberry"},next:{title:"Training Your Model",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 5 - Custom Model Development and Deployment/Training Your Model"}},l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Install Ollama",id:"install-ollama",level:2},{value:"Install Python IDE",id:"install-python-ide",level:2},{value:"Check wether Ollama is installed",id:"check-wether-ollama-is-installed",level:2},{value:"Use Ollama API with Python",id:"use-ollama-api-with-python",level:2},{value:"Use ollama.chat()",id:"use-ollamachat",level:2},{value:"Getting an image description",id:"getting-an-image-description",level:2},{value:"Function Calling",id:"function-calling",level:2},{value:"But what exactly is &quot;function calling&quot;?",id:"but-what-exactly-is-function-calling",level:2},{value:"Resources",id:"resources",level:2}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"ollama-python-library",children:"Ollama Python Library"}),"\n",(0,a.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(n.p,{children:"So far, we have explored SLMs' chat capability using the command line on a terminal. However, we want to integrate those models into our projects, so Python seems to be the right path. The good news is that Ollama has such a library."}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://github.com/ollama/ollama-python",children:"Ollama Python library"})," simplifies interaction with advanced LLM models, enabling more sophisticated responses and capabilities, besides providing the easiest way to integrate Python 3.8+ projects with ",(0,a.jsx)(n.a,{href:"https://github.com/ollama/ollama",children:"Ollama."})]}),"\n",(0,a.jsxs)(n.p,{children:["For a better understanding of how to create apps using Ollama with Python, we can follow ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/@technovangelist",children:"Matt Williams's videos"}),", as the one below:"]}),"\n",(0,a.jsx)("iframe",{width:"1920",height:"932",src:"https://www.youtube.com/embed/_4K20tOsXK8",title:"Unlocking The Power Of AI: Creating Python Apps With Ollama!",frameborder:"0",allow:"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share",referrerpolicy:"strict-origin-when-cross-origin",allowfullscreen:!0}),"\n",(0,a.jsx)(n.h2,{id:"install-ollama",children:"Install Ollama"}),"\n",(0,a.jsx)(n.p,{children:"In the terminal, run the command:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install ollama\n"})}),"\n",(0,a.jsx)(n.h2,{id:"install-python-ide",children:"Install Python IDE"}),"\n",(0,a.jsxs)(n.p,{children:["We will need a text editor or an IDE to create a Python script. If you run the Raspberry OS on a desktop, several options, such as Thonny and Geany, have already been installed by default (accessed by ",(0,a.jsx)(n.code,{children:"[Menu][Programming]"}),"). You can download other IDEs, such as Visual Studio Code, from ",(0,a.jsx)(n.code,{children:"[Menu][Recommended Software]"}),". When the window pops up, go to ",(0,a.jsx)(n.code,{children:"[Programming]"}),", select the option of your choice, and press ",(0,a.jsx)(n.code,{children:"[Apply]"}),"."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(3305).Z+"",width:"656",height:"426"})}),"\n",(0,a.jsx)(n.p,{children:"If you prefer using Jupyter Notebook for development:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install jupyter\njupyter notebook --generate-config\n"})}),"\n",(0,a.jsx)(n.p,{children:"To run Jupyter Notebook, run the command (change the IP address for yours):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"jupyter notebook --ip=192.168.4.209 --no-browser\n"})}),"\n",(0,a.jsx)(n.p,{children:"On the terminal, you can see the local URL address to open the notebook:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(5265).Z+"",width:"1032",height:"581"})}),"\n",(0,a.jsx)(n.p,{children:"We can access it from another computer by entering the Raspberry Pi's IP address and the provided token in a web browser (we should copy it from the terminal)."}),"\n",(0,a.jsx)(n.p,{children:"In our working directory in the Raspi, we will create a new Python 3 notebook."}),"\n",(0,a.jsx)(n.p,{children:"Let's enter with a very simple script to verify the installed models:"}),"\n",(0,a.jsx)(n.h2,{id:"check-wether-ollama-is-installed",children:"Check wether Ollama is installed"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import ollama\nollama.list()\n"})}),"\n",(0,a.jsx)(n.p,{children:"All the models will be printed as a dictionary, for example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"  {'name': 'gemma2:2b',\n   'model': 'gemma2:2b',\n   'modified_at': '2024-09-24T19:30:40.053898094+01:00',\n   'size': 1629518495,\n   'digest': '8ccf136fdd5298f3ffe2d69862750ea7fb56555fa4d5b18c04e3fa4d82ee09d7',\n   'details': {'parent_model': '',\n    'format': 'gguf',\n    'family': 'gemma2',\n    'families': ['gemma2'],\n    'parameter_size': '2.6B',\n    'quantization_level': 'Q4_0'}}]}\n"})}),"\n",(0,a.jsx)(n.h2,{id:"use-ollama-api-with-python",children:"Use Ollama API with Python"}),"\n",(0,a.jsxs)(n.p,{children:["Let's repeat one of the questions that we did before, but now using ",(0,a.jsx)(n.code,{children:"ollama.generate()"})," from Ollama python library. This API will generate a response for the given prompt with the provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"MODEL = 'gemma2:2b'\nPROMPT = 'What is the capital of France?'\n\nres = ollama.generate(model=MODEL, prompt=PROMPT)\nprint (res)\n"})}),"\n",(0,a.jsx)(n.p,{children:"In case you are running the code as a Python script, you should save it, for example, test_ollama.py. You can use the IDE to run it or do it directly on the terminal. Also, remember that you should always call the model and define it when running a stand-alone script."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"python test_ollama.py\n"})}),"\n",(0,a.jsx)(n.p,{children:"As a result, we will have the model response in a JSON format:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"{'model': 'gemma2:2b', 'created_at': '2024-09-25T14:43:31.869633807Z', \n'response': 'The capital of France is **Paris**. \ud83c\uddeb\ud83c\uddf7 \\n', 'done': True, \n'done_reason': 'stop', 'context': [106, 1645, 108, 1841, 603, 573, 6037, 576,\n6081, 235336, 107, 108, 106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, \n168428, 235248, 244304, 241035, 235248, 108], 'total_duration': 24259469458, \n'load_duration': 19830013859, 'prompt_eval_count': 16, 'prompt_eval_duration': \n1908757000, 'eval_count': 14, 'eval_duration': 2475410000}\n"})}),"\n",(0,a.jsx)(n.p,{children:"As we can see, several pieces of information are generated, such as:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"response"}),":  the main output text generated by the model in response to our prompt."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.code,{children:"The capital of France is **Paris**. \ud83c\uddeb\ud83c\uddf7"})}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"context"}),": the token IDs representing the input and context used by the model. Tokens are numerical representations of text used for processing by the language model."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"[106, 1645, 108, 1841, 603, 573, 6037, 576, 6081, 235336, 107, 108,"}),"\n",(0,a.jsx)(n.code,{children:"106, 2516, 108, 651, 6037, 576, 6081, 603, 5231, 29437, 168428,"}),"\n",(0,a.jsx)(n.code,{children:" 235248, 244304, 241035, 235248, 108]"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The Performance Metrics:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"total_duration"}),": The total time taken for the operation in nanoseconds. In this case, approximately 24.26 seconds."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"load_duration"}),": The time taken to load the model or components in nanoseconds. About 19.38 seconds."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"prompt_eval_duration"}),": The time taken to evaluate the prompt in nanoseconds. Around 1.9.0 seconds."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"eval_count"}),": The number of tokens evaluated during the generation. Here, 14 tokens."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"eval_duration"}),": The time taken for the model to generate the response in nanoseconds. Approximately 2.5 seconds."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"But, what we want is the plain 'response' and, perhaps for analysis, the total duration of the inference, so let's change the code to extract it from the dictionary:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"print(f\"\\n{res['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now, we got:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"The capital of France is **Paris**. \ud83c\uddeb\ud83c\uddf7 \n\n [INFO] Total Duration: 24.26 seconds\n"})}),"\n",(0,a.jsx)(n.h2,{id:"use-ollamachat",children:"Use ollama.chat()"}),"\n",(0,a.jsxs)(n.p,{children:["Another way to get our response is to use ",(0,a.jsx)(n.code,{children:"ollama.chat()"}),", which generates the next message in a chat with a provided model. This is a streaming endpoint, so a series of responses will occur. Streaming can be disabled using ",(0,a.jsx)(n.code,{children:'"stream": false'}),". The final response object will also include statistics and additional data from the request."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"PROMPT_1 = 'What is the capital of France?'\n\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\n"})}),"\n",(0,a.jsx)(n.p,{children:"The answer is the same as before."}),"\n",(0,a.jsxs)(n.p,{children:["An important consideration is that by using ",(0,a.jsx)(n.code,{children:"ollama.generate()"}),', the response is "clear" from the model\'s "memory" after the end of inference (only used once), but If we want to keep a conversation, we must use ',(0,a.jsx)(n.code,{children:"ollama.chat()"}),". Let's see it in action:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"PROMPT_1 = 'What is the capital of France?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},])\nresp_1 = response['message']['content']\nprint(f\"\\n{resp_1}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n\nPROMPT_2 = 'and of Italy?'\nresponse = ollama.chat(model=MODEL, messages=[\n{'role': 'user','content': PROMPT_1,},\n{'role': 'assistant','content': resp_1,},\n{'role': 'user','content': PROMPT_2,},])\nresp_2 = response['message']['content']\nprint(f\"\\n{resp_2}\")\nprint(f\"\\n [INFO] Total Duration: {(response['total_duration']/1e9):.2f} seconds\")\n"})}),"\n",(0,a.jsx)(n.p,{children:"In the above code, we are running two queries, and the second prompt considers the result of the first one."}),"\n",(0,a.jsx)(n.p,{children:"Here is how the model responded:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"The capital of France is **Paris**. \ud83c\uddeb\ud83c\uddf7 \n\n [INFO] Total Duration: 2.82 seconds\n\nThe capital of Italy is **Rome**. \ud83c\uddee\ud83c\uddf9 \n\n [INFO] Total Duration: 4.46 seconds\n"})}),"\n",(0,a.jsx)(n.h2,{id:"getting-an-image-description",children:"Getting an image description"}),"\n",(0,a.jsxs)(n.p,{children:["In the same way that we have used the ",(0,a.jsx)(n.code,{children:"LlaVa-PHI-3"})," model with the command line to analyze an image, the same can be done here with Python. Let's use the same image of Paris, but now with the ",(0,a.jsx)(n.code,{children:"ollama.generate()"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"MODEL = 'llava-phi3:3.8b'\nPROMPT = \"Describe this picture\"\n\nwith open('image_test_1.jpg', 'rb') as image_file:\n    img = image_file.read()\n\nresponse = ollama.generate(\n    model=MODEL,\n    prompt=PROMPT,\n    images= [img]\n)\nprint(f\"\\n{response['response']}\")\nprint(f\"\\n [INFO] Total Duration: {(res['total_duration']/1e9):.2f} seconds\")\n"})}),"\n",(0,a.jsx)(n.p,{children:"Here is the result:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"This image captures the iconic cityscape of Paris, France. The vantage point \nis high, providing a panoramic view of the Seine River that meanders through \nthe heart of the city. Several bridges arch gracefully over the river, \nconnecting different parts of the city. The Eiffel Tower, an iron lattice \nstructure with a pointed top and two antennas on its summit, stands tall in the \nbackground, piercing the sky. It is painted in a light gray color, contrasting \nagainst the blue sky speckled with white clouds.\n\nThe buildings that line the river are predominantly white or beige, their uniform\ncolor palette broken occasionally by red roofs peeking through. The Seine River \nitself appears calm and wide, reflecting the city's architectural beauty in its \nsurface. On either side of the river, trees add a touch of green to the urban \nlandscape.\n\nThe image is taken from an elevated perspective, looking down on the city. This \nviewpoint allows for a comprehensive view of Paris's beautiful architecture and \nlayout. The relative positions of the buildings, bridges, and other structures \ncreate a harmonious composition that showcases the city's charm.\n\nIn summary, this image presents a serene day in Paris, with its architectural \nmarvels - from the Eiffel Tower to the river-side buildings - all bathed in soft \ncolors under a clear sky.\n\n [INFO] Total Duration: 256.45 seconds\n"})}),"\n",(0,a.jsx)(n.p,{children:"The model took about 4 minutes (256.45 s) to return with a detailed image description."}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:["In the ",(0,a.jsx)(n.a,{href:"https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb",children:"10-Ollama_Python_Library"})," notebook, it is possible to find the experiments with the Ollama Python library."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"function-calling",children:"Function Calling"}),"\n",(0,a.jsxs)(n.p,{children:["So far, we can see that, with the model's (\"response\") answer to a variable, we can efficiently work with it, integrating it into real-world projects. However, a big problem is that the model can respond differently to the same prompt. Let's say that what we want, as the model's response in the last examples, is only the name of a given country's capital and its coordinates, nothing more, even with very verbose models such as the Microsoft Phi. We can use the ",(0,a.jsx)(n.code,{children:"Ollama function's calling"})," to guarantee the same answers, which is perfectly compatible with OpenAI API."]}),"\n",(0,a.jsx)(n.h2,{id:"but-what-exactly-is-function-calling",children:'But what exactly is "function calling"?'}),"\n",(0,a.jsx)(n.p,{children:"In modern artificial intelligence, function calling with Large Language Models (LLMs) allows these models to perform actions beyond generating text. By integrating with external functions or APIs, LLMs can access real-time data, automate tasks, and interact with various systems."}),"\n",(0,a.jsx)(n.p,{children:"For instance, instead of merely responding to a query about the weather, an LLM can call a weather API to fetch the current conditions and provide accurate, up-to-date information. This capability enhances the relevance and accuracy of the model's responses and makes it a powerful tool for driving workflows and automating processes, transforming it into an active participant in real-world applications."}),"\n",(0,a.jsxs)(n.p,{children:["For more details about Function Calling, please see this video made by ",(0,a.jsx)(n.a,{href:"https://www.youtube.com/@MervinPraison",children:"Marvin Prison"}),":"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.a,{href:"https://www.youtube.com/embed/eHfMCtlsb1o",children:"https://www.youtube.com/embed/eHfMCtlsb1o"})}),"\n",(0,a.jsx)(n.h2,{id:"resources",children:"Resources"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/10-Ollama_Python_Library.ipynb",children:"10-Ollama_Python_Library notebook"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/20-Ollama_Function_Calling.ipynb",children:"20-Ollama_Function_Calling notebook"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/Mjrovai/EdgeML-with-Raspberry-Pi/blob/main/OLLAMA_SLMs/30-Function_Calling_with_images.ipynb",children:"30-Function_Calling_with_images notebook"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},5265:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/jupyter-3969a594877c2c33d7fa8cf01d2c4ac6.png"},3305:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/menu-f0b5ffedb3bb871916f935c40bf9f494.png"},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(7294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);