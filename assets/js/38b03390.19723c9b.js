"use strict";(self.webpackChunktutorial_of_ai_kit=self.webpackChunktutorial_of_ai_kit||[]).push([[828],{3393:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>d,frontMatter:()=>o,metadata:()=>c,toc:()=>l});var t=r(5893),i=r(1151);const o={sidebar_position:1},s="Introduction to Pytorch in Raspberry Pi Environment",c={id:"Chapter_2-Configuring_the_RaspberryPi_Environment/Introduction_to_Pytorch_in_Raspberry_Pi_Environment",title:"Introduction to Pytorch in Raspberry Pi Environment",description:"What is PyTorch?",source:"@site/../articles/Chapter_2-Configuring_the_RaspberryPi_Environment/Introduction_to_Pytorch_in_Raspberry_Pi_Environment.md",sourceDirName:"Chapter_2-Configuring_the_RaspberryPi_Environment",slug:"/Chapter_2-Configuring_the_RaspberryPi_Environment/Introduction_to_Pytorch_in_Raspberry_Pi_Environment",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_2-Configuring_the_RaspberryPi_Environment/Introduction_to_Pytorch_in_Raspberry_Pi_Environment",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Chapter 2 - Configuring the Raspberry Pi Environment",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_2-Configuring_the_RaspberryPi_Environment/"},next:{title:"Introduction to TensorFlow in Raspberry Pi Environment",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter_2-Configuring_the_RaspberryPi_Environment/Introduction_to_TensorFlow_in_Raspberry_Pi_Environment"}},a={},l=[{value:"What is PyTorch?",id:"what-is-pytorch",level:2},{value:"Brief History",id:"brief-history",level:3},{value:"Why Use PyTorch?",id:"why-use-pytorch",level:3},{value:"What Are Dynamic Computation Graphs?",id:"what-are-dynamic-computation-graphs",level:3},{value:"Who Uses PyTorch?",id:"who-uses-pytorch",level:3},{value:"PyTorch vs TensorFlow: Feature Comparison",id:"pytorch-vs-tensorflow-feature-comparison",level:2},{value:"PyTorch vs TensorFlow: Feature Comparison",id:"pytorch-vs-tensorflow-feature-comparison-1",level:2},{value:"What is QNNPACK?",id:"what-is-qnnpack",level:2},{value:"Setting Up the Environment for PyTorch Classification",id:"setting-up-the-environment-for-pytorch-classification",level:2},{value:"Python Code (pytorch_test.py)",id:"python-code-pytorch_testpy",level:2},{value:"How to Run the Code",id:"how-to-run-the-code",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"introduction-to-pytorch-in-raspberry-pi-environment",children:"Introduction to Pytorch in Raspberry Pi Environment"}),"\n",(0,t.jsx)(n.h2,{id:"what-is-pytorch",children:"What is PyTorch?"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"pytorchlogo",src:r(7298).Z+"",width:"2500",height:"1250"})}),"\n",(0,t.jsx)(n.p,{children:"PyTorch is an open-source machine learning framework developed by Facebook's AI Research lab (FAIR). It is known for its flexibility, dynamic computation graphs, and strong community support. PyTorch simplifies the development of deep learning models, making it a popular choice for researchers and practitioners alike."}),"\n",(0,t.jsx)(n.h3,{id:"brief-history",children:"Brief History"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"2016"}),": PyTorch was released by Facebook as an open-source library, combining features of Torch (a Lua-based framework) and Python for easy usability."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"2019"}),": Gained significant momentum when Facebook partnered with Microsoft to create ONNX (Open Neural Network Exchange) for model interoperability."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"2022"}),": PyTorch became part of the PyTorch Foundation, ensuring community-driven development."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"why-use-pytorch",children:"Why Use PyTorch?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Dynamic Graphs"}),": PyTorch uses dynamic computation graphs, allowing flexibility in building and debugging models."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pythonic"}),": Integrates seamlessly with Python, making it intuitive for Python developers."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Community Support"}),": A vibrant ecosystem with numerous tutorials, forums, and open-source projects."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Accelerated Research"}),": Its ease of use accelerates model experimentation and implementation."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"what-are-dynamic-computation-graphs",children:"What Are Dynamic Computation Graphs?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"A computation graph represents the operations performed on data (e.g., tensors) in a deep learning model."}),"\n",(0,t.jsxs)(n.li,{children:["Dynamic Graphs (PyTorch)",":The"," computation graph is built on the fly as operations are executed.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each forward pass can construct a different graph, allowing for greater flexibility and adaptability."}),"\n",(0,t.jsx)(n.li,{children:"You don\u2019t need to define the entire graph beforehand; it evolves during runtime."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"who-uses-pytorch",children:"Who Uses PyTorch?"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Research Organizations"}),": MIT, Stanford, OpenAI, and FAIR."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Companies"}),": Facebook (Meta), Tesla (autonomous driving), Disney (AI for animation), and Microsoft."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Domains"}),": Used in computer vision, natural language processing, reinforcement learning, and more."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pytorch-vs-tensorflow-feature-comparison",children:"PyTorch vs TensorFlow: Feature Comparison"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"pytorchvstf",src:r(1136).Z+"",width:"1461",height:"766"})}),"\n",(0,t.jsx)(n.h2,{id:"pytorch-vs-tensorflow-feature-comparison-1",children:"PyTorch vs TensorFlow: Feature Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"Feature"})}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"PyTorch"})}),(0,t.jsx)(n.th,{children:(0,t.jsx)(n.strong,{children:"TensorFlow"})})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Computation Graph"})}),(0,t.jsx)(n.td,{children:"Dynamic (easier for debugging)"}),(0,t.jsx)(n.td,{children:"Static (optimized for deployment)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Ease of Use"})}),(0,t.jsx)(n.td,{children:"Intuitive and Pythonic"}),(0,t.jsx)(n.td,{children:"Requires a steeper learning curve"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Community"})}),(0,t.jsx)(n.td,{children:"Popular in academia and research"}),(0,t.jsx)(n.td,{children:"Widely used in production and enterprises"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Frameworks Built On"})}),(0,t.jsx)(n.td,{children:"Lightning, Detectron2, Hugging Face"}),(0,t.jsx)(n.td,{children:"Keras, TFX, TensorFlow Lite"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Deployment"})}),(0,t.jsx)(n.td,{children:"TorchServe, ONNX"}),(0,t.jsx)(n.td,{children:"TensorFlow Serving, TensorFlow.js"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Performance"})}),(0,t.jsx)(n.td,{children:"Efficient but depends on optimization"}),(0,t.jsx)(n.td,{children:"Better optimization for large-scale tasks"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"what-is-qnnpack",children:"What is QNNPACK?"}),"\n",(0,t.jsx)(n.p,{children:"QNNPACK (Quantized Neural Network PACKage) is a high-performance kernel library developed by Facebook for running quantized neural networks efficiently on ARM CPUs. It is optimized for low-power devices, such as mobile phones and Raspberry Pi, and is a critical component for executing PyTorch's quantized models. It supports operations like convolutions, fully connected layers, and more, tailored for low-precision inference."}),"\n",(0,t.jsx)(n.h2,{id:"setting-up-the-environment-for-pytorch-classification",children:"Setting Up the Environment for PyTorch Classification"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Create a Virtual Environment"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"mkdir my_pytorch_course\ncd my_pytorch_course\npython -m venv --system-site-packages env\nsource env/bin/activate\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Install Required Libraries"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision torchaudio opencv-python numpy\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Prepare Your Directory"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create a folder on your Desktop named pytorch."}),"\n",(0,t.jsxs)(n.li,{children:["Inside the pytorch folder, create the following files:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"pytorch_test.py (for your Python code)."}),"\n",(0,t.jsx)(n.li,{children:"imagenet-classes.txt (contains ImageNet class labels)."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"folder",src:r(9962).Z+"",width:"1027",height:"776"})}),"\n",(0,t.jsx)(n.h2,{id:"python-code-pytorch_testpy",children:"Python Code (pytorch_test.py)"}),"\n",(0,t.jsx)(n.p,{children:"Copy the provided Python code into the file pytorch_test.py:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:'import time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.set_num_threads(torch.get_num_threads())\n\n# Ensure qnnpack backend is used for quantized models\ntorch.backends.quantized.engine = \'qnnpack\'\n\n# Load the ImageNet class labels\nwith open("imagenet-classes.txt", "r") as f:\n    classes = [line.strip() for line in f.readlines()]\n\n# Initialize webcam\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\n# Preprocessing pipeline\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Load MobileNetV2 quantized model\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\nnet = torch.jit.script(net)  # Optimize model for inference\n\n# Performance logging\nstarted = time.time()\nlast_logged = time.time()\nframe_count = 0\n\n# Real-time inference\nwith torch.no_grad():\n    while True:\n        # Read frame from webcam\n        ret, frame = cap.read()\n        if not ret:\n            print("Failed to capture frame. Exiting...")\n            break\n\n        # Convert BGR to RGB\n        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Preprocess image\n        input_tensor = preprocess(image)\n        input_batch = input_tensor.unsqueeze(0)\n\n        # Perform inference\n        output = net(input_batch)\n        probabilities = output[0].softmax(dim=0)\n\n        # Get top-10 predictions\n        top = list(enumerate(probabilities))\n        top.sort(key=lambda x: x[1], reverse=True)\n        top_predictions = [(classes[idx], val.item()) for idx, val in top[:3]]\n\n        # Display predictions on the frame\n        for i, (label, prob) in enumerate(top_predictions):\n            text = f"{prob * 100:.2f}% {label}"\n            cv2.putText(frame, text, (10, 25 + i * 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n\n        # Show the frame\n        cv2.imshow("Real-time Object Recognition", frame)\n\n        # Log fps\n        frame_count += 1\n        now = time.time()\n        if now - last_logged > 1:\n            print(f"{frame_count / (now - last_logged):.2f} fps")\n            last_logged = now\n            frame_count = 0\n\n        # Exit on pressing \'q\'\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n\n\n'})}),"\n",(0,t.jsx)(n.h2,{id:"how-to-run-the-code",children:"How to Run the Code"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Navigate to the pytorch directory"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"cd /home/pi/Desktop/pytorch\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Run the Python script"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python pytorch_test.py\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Check Output"})}),"\n",(0,t.jsx)(n.p,{children:"A window will open showing the real-time webcam feed.The top-3 predictions (with confidence percentages) will be displayed on the video feed."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{alt:"result",src:r(3488).Z+"",width:"1022",height:"730"})}),"\n",(0,t.jsx)(n.p,{children:"Futher references :"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://pytorch.org/docs/stable/index.html",children:"Pytorch Documentation"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"https://www.learnpytorch.io/",children:"Pytorch Course"})})]})}function d(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},9962:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/folder_torch-60a79a6463bbf4d087db0c134240b68c.PNG"},7298:(e,n,r)=>{r.d(n,{Z:()=>t});const t="data:image/svg+xml;base64,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"},1136:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/tfvstorch-5bc564f84587b3f93726ab5e2a299eb3.PNG"},3488:(e,n,r)=>{r.d(n,{Z:()=>t});const t=r.p+"assets/images/torch_results-d9341f4cbf8c44c75e1dbbfb674a12a5.PNG"},1151:(e,n,r)=>{r.d(n,{Z:()=>c,a:()=>s});var t=r(7294);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);