"use strict";(self.webpackChunktutorial_of_ai_kit=self.webpackChunktutorial_of_ai_kit||[]).push([[616],{794:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>o,toc:()=>c});var n=i(5893),s=i(1151);const r={sidebar_position:3},a="Introduction of Convolutional Neural Network",o={id:"Chapter 1 - Introduction to AI/Introduction_of_Convolutional_Neural_Network",title:"Introduction of Convolutional Neural Network",description:"Convolution Neural Network",source:"@site/../articles/Chapter 1 - Introduction to AI/Introduction_of_Convolutional_Neural_Network.md",sourceDirName:"Chapter 1 - Introduction to AI",slug:"/Chapter 1 - Introduction to AI/Introduction_of_Convolutional_Neural_Network",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 1 - Introduction to AI/Introduction_of_Convolutional_Neural_Network",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Introduction to DNN",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 1 - Introduction to AI/Introduction_to_DNN"},next:{title:"Mastering Computer Vision with Seeed Studio",permalink:"/Tutorial-of-AI-Kit-with-Raspberry-Pi-From-Zero-to-Hero/docs/Chapter 1 - Introduction to AI/Introduction_of_Computer_Vision"}},l={},c=[{value:"Convolution Neural Network",id:"convolution-neural-network",level:2},{value:"Why CNN is Different from DNN",id:"why-cnn-is-different-from-dnn",level:3},{value:"Basic CNN Structure",id:"basic-cnn-structure",level:3},{value:"What are the Popular Image Classification architectures?",id:"what-are-the-popular-image-classification-architectures",level:2},{value:"Object Detection",id:"object-detection",level:2},{value:"Object Detection Architectures",id:"object-detection-architectures",level:2}];function d(e){const t={a:"a",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.h1,{id:"introduction-of-convolutional-neural-network",children:"Introduction of Convolutional Neural Network"}),"\n",(0,n.jsx)(t.h2,{id:"convolution-neural-network",children:"Convolution Neural Network"}),"\n",(0,n.jsx)(t.p,{children:"Convolutional Neural Networks (CNNs) are a specialized type of deep learning model designed to process and analyze visual data, such as images and videos. They are particularly effective at recognizing patterns and spatial hierarchies within images, making them ideal for tasks like object detection, image classification, and facial recognition. Unlike traditional neural networks, CNNs use convolutional layers to automatically learn local features, which allows them to excel in capturing visual information. This makes CNNs the state-of-the-art approach for many image-related AI applications."}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"CNN",src:i(7782).Z+"",width:"1400",height:"749"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.a,{href:"https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53",children:"References"})}),"\n",(0,n.jsx)(t.h3,{id:"why-cnn-is-different-from-dnn",children:"Why CNN is Different from DNN"}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"CNN is designed for image data"}),":","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CNNs are specialized for processing and analyzing images by automatically learning patterns like edges, shapes, and textures."}),"\n",(0,n.jsx)(t.li,{children:"DNNs are more general and can be used for various tasks, but they don't excel at spatial pattern recognition like CNNs."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Local feature learning vs. Global feature learning"}),":","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CNN uses convolutional layers that focus on small regions of an image (local features), capturing spatial relationships."}),"\n",(0,n.jsx)(t.li,{children:"DNNs use fully connected layers that consider the entire input (global features), making them less effective for image data."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"CNN uses fewer parameters"}),":","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CNN\u2019s convolutional layers are sparsely connected (not every neuron connects to every input), reducing the number of parameters and computation."}),"\n",(0,n.jsx)(t.li,{children:"DNN\u2019s layers are fully connected, which increases the number of parameters, making them less efficient for image processing tasks."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:[(0,n.jsx)(t.strong,{children:"Better for spatial data"}),":","\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"CNN is excellent for image-related tasks like object detection and classification because it recognizes spatial hierarchies in data."}),"\n",(0,n.jsx)(t.li,{children:"DNNs, although effective, do not naturally handle spatial information in the same way."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(t.h3,{id:"basic-cnn-structure",children:"Basic CNN Structure"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Convolution Layer:"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Extracts features from the image by applying filters (kernels) that detect patterns like edges, textures, etc."}),"\n",(0,n.jsx)(t.li,{children:"Output: Feature maps that represent learned patterns."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"CNN",src:i(2439).Z+"",width:"1095",height:"447"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.a,{href:"https://compneuro.neuromatch.io/tutorials/W1D5_DeepLearning/student/W1D5_Tutorial2.html",children:"References"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Pooling Layer"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"Reduces the size of feature maps (down-sampling) to make computation more efficient."}),"\n",(0,n.jsx)(t.li,{children:"Common technique: Max-pooling, where the maximum value in a region is taken to reduce data size."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"CNN",src:i(9606).Z+"",width:"728",height:"324"})}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Fully Connected Layer (FC)"}),":"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"A traditional layer where all neurons are connected to every neuron in the previous layer."}),"\n",(0,n.jsx)(t.li,{children:"Helps in combining the features extracted by convolution layers to make final predictions."}),"\n"]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.strong,{children:"Output Layer"}),":"]}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"The final layer where the model gives its prediction, such as identifying the object in an image."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.a,{href:"https://www.youtube.com/watch?v=CXOGvCMLrkA",children:"References"})}),"\n",(0,n.jsx)(t.h2,{id:"what-are-the-popular-image-classification-architectures",children:"What are the Popular Image Classification architectures?"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"LeNet"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"LeNet, developed by Yann LeCun in 1998, is one of the first CNN models, designed for handwritten digit recognition (like the MNIST dataset)."}),"\n",(0,n.jsx)(t.li,{children:"It has a simple structure with two convolutional layers followed by pooling layers, and fully connected layers for classification."}),"\n",(0,n.jsx)(t.li,{children:"LeNet laid the foundation for modern CNNs and is used in early computer vision tasks like digit classification."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"CNN",src:i(401).Z+"",width:"800",height:"235"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"VGG16"})}),"\n",(0,n.jsxs)(t.ul,{children:["\n",(0,n.jsx)(t.li,{children:"VGG16, created by the Visual Geometry Group at Oxford, is a deep CNN with 16 layers, primarily used for image classification tasks."}),"\n",(0,n.jsx)(t.li,{children:"It uses small 3x3 convolution filters and stacks multiple layers together to capture detailed features, followed by fully connected layers."}),"\n",(0,n.jsx)(t.li,{children:"VGG16 is popular for its simplicity and effectiveness in large-scale image classification and object detection tasks."}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"CNN",src:i(3401).Z+"",width:"827",height:"662"})}),"\n",(0,n.jsx)(t.p,{children:"Here\u2019s a comparative chart of popular CNN architectures"}),"\n",(0,n.jsx)(t.p,{children:"Here\u2019s a comparative chart of popular CNN architectures"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Architecture"}),(0,n.jsx)(t.th,{children:"Year"}),(0,n.jsx)(t.th,{children:"Key Features"}),(0,n.jsx)(t.th,{children:"Use Cases"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"LeNet"}),(0,n.jsx)(t.td,{children:"1998"}),(0,n.jsx)(t.td,{children:"Simple 5-layer network, uses Tanh"}),(0,n.jsx)(t.td,{children:"Handwritten digit recognition"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"VGG16"}),(0,n.jsx)(t.td,{children:"2014"}),(0,n.jsx)(t.td,{children:"Deep network with 16 layers, uniform 3x3 filters"}),(0,n.jsx)(t.td,{children:"Image classification, object detection"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"ResNet"}),(0,n.jsx)(t.td,{children:"2015"}),(0,n.jsx)(t.td,{children:"Residual connections, deep network with skip connections"}),(0,n.jsx)(t.td,{children:"Image classification, object detection, face recognition"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"MobileNet"}),(0,n.jsx)(t.td,{children:"2017"}),(0,n.jsx)(t.td,{children:"Lightweight network, depthwise separable convolutions"}),(0,n.jsx)(t.td,{children:"Mobile and edge applications, real-time object detection"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"EfficientNet"}),(0,n.jsx)(t.td,{children:"2019"}),(0,n.jsx)(t.td,{children:"Scaled CNN models, compound scaling for accuracy vs. efficiency"}),(0,n.jsx)(t.td,{children:"Image classification, object detection, mobile applications"})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"object-detection",children:"Object Detection"}),"\n",(0,n.jsx)(t.p,{children:"Object detection is a computer vision technique that identifies and localizes objects within images or video by marking them with bounding boxes. Unlike simple image classification, which only labels an entire image, object detection provides spatial information, detecting multiple objects and their positions simultaneously. It enables applications ranging from autonomous driving to real-time surveillance by combining classification and localization tasks. This makes it a crucial step toward understanding visual scenes in depth."}),"\n",(0,n.jsx)(t.h2,{id:"object-detection-architectures",children:"Object Detection Architectures"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Two-Stage Detectors"})}),"\n",(0,n.jsx)(t.p,{children:"Two-stage detectors work in two main steps. First, they generate region proposals\u2014likely areas in the image where objects might be located. Then, in the second stage, they refine these proposals and classify them into specific object categories. This approach balances accuracy by focusing on the most relevant parts of an image, which improves detection but can slow down processing."}),"\n",(0,n.jsx)(t.p,{children:"Ex: R-CNN, Fast RCNN"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"RCNN",src:i(9119).Z+"",width:"988",height:"366"})}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.strong,{children:"Single-Stage Detectors"})}),"\n",(0,n.jsx)(t.p,{children:"Single-stage detectors streamline the process by predicting bounding boxes and class labels in a single pass over the image. Instead of generating region proposals first, they treat object detection as a dense prediction problem\u2014examining the entire image at once, making them faster than two-stage methods. These models are generally more suitable for real-time applications, though sometimes less accurate."}),"\n",(0,n.jsx)(t.p,{children:"Ex: SSD and Yolo"}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.img,{alt:"Yolo",src:i(550).Z+"",width:"850",height:"336"})}),"\n",(0,n.jsx)(t.p,{children:"Here\u2019s a chart comparing popular object detection architectures"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:(0,n.jsx)(t.strong,{children:"Architecture"})}),(0,n.jsx)(t.th,{children:(0,n.jsx)(t.strong,{children:"Year"})}),(0,n.jsx)(t.th,{children:(0,n.jsx)(t.strong,{children:"Key Features"})}),(0,n.jsx)(t.th,{children:(0,n.jsx)(t.strong,{children:"Use Cases"})})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"R-CNN"})}),(0,n.jsx)(t.td,{children:"2014"}),(0,n.jsx)(t.td,{children:"Two-stage detector, Selective search to generate region proposals, Slow and high memory usage"}),(0,n.jsx)(t.td,{children:"Object detection in high-resolution images (e.g., satellite, medical imaging)"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"SSD (Single Shot Detector)"})}),(0,n.jsx)(t.td,{children:"2016"}),(0,n.jsx)(t.td,{children:"Single-stage detector, Multi-scale feature maps, Balances speed and accuracy"}),(0,n.jsx)(t.td,{children:"Real-time detection, self-driving cars, security cameras"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"YOLO (You Only Look Once)"})}),(0,n.jsx)(t.td,{children:"2016"}),(0,n.jsx)(t.td,{children:"Single-stage detector, Divides image into grid cells, Fast, optimized for real-time applications"}),(0,n.jsx)(t.td,{children:"Surveillance, autonomous vehicles, video analysis"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"SSD_MobileNet"})}),(0,n.jsx)(t.td,{children:"2017"}),(0,n.jsx)(t.td,{children:"MobileNet backbone for lightweight, mobile-friendly performance, Suitable for edge devices"}),(0,n.jsx)(t.td,{children:"Mobile and IoT devices, embedded systems, robotics"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.strong,{children:"EfficientDet"})}),(0,n.jsx)(t.td,{children:"2020"}),(0,n.jsx)(t.td,{children:"EfficientNet backbone, Uses compound scaling, High accuracy with lower computation"}),(0,n.jsx)(t.td,{children:"Real-time applications on limited hardware, drones, edge AI"})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},9119:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/RCNN-cd7676726ac87742e454603d0500819f.png"},550:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/YOLO-b639f583cbe25bc94da50e67f5c63d68.png"},7782:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/cnn1-961d96fec428b95931a0424daf1aa17e.jpg"},2439:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/conv-af209fa139c4da1f81ca247c73d8b7ce.gif"},401:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/lenet-b95858d9d5389034778eab61df2bcce4.png"},9606:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/maxpool-7f194a917842535e5569ce3b2bba5ce7.gif"},3401:(e,t,i)=>{i.d(t,{Z:()=>n});const n=i.p+"assets/images/vgg16-80d492b97b504fb136cf14a3253f6cb9.png"},1151:(e,t,i)=>{i.d(t,{Z:()=>o,a:()=>a});var n=i(7294);const s={},r=n.createContext(s);function a(e){const t=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),n.createElement(r.Provider,{value:t},e.children)}}}]);